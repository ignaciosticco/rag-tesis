import streamlit as st
import os
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

#### Configuracion de la pagina web ####

st.title("üöÄ ChatBOT de la Tesis Doctoral de Ignacio Sticco üöÄ")
st.write("#### Pregunta:")
pregunta = st.text_input("Hac√© tu pregunta")
st.write("#### Respuesta:")


#### Importacion y chunkeo de la base de datos ####

loader = TextLoader("./tesis_doctoral.txt", encoding='utf-8')
text_documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
documents = text_splitter.split_documents(text_documents)
embeddings = OpenAIEmbeddings()
vectorstore = DocArrayInMemorySearch.from_documents(documents, embeddings) # Embed the chunked documents


#### Configuracion de la cadena ####
model = ChatOpenAI(openai_api_key=st.secrets["OPENAI_API_KEY"], model="gpt-3.5-turbo")
parser = StrOutputParser()

template = """
Respond√© las preguntas basandote en el siguiente contexto. Si no podes responder una pregunta. 
Responde: "Necesito m√°s informaci√≥n para responder esta pregunta.". 

Si te preguntan quienes son los directores respond√©: Claudio Dorso y Guillermo Frank.

Si te preguntan cu√°les son las conclusiones respond√© algo semejante a este texto: 
"La conclusi√≥n es que los vest√≠bulos mejoran las evacuaciones de emergencia por dos motivos. 
Por un lado, incrementan el flujo de evacuaci√≥n, por otro lado, disminuyen la presi√≥n que soportan los 
individuos. El aumento del flujo se debe al hecho que los vest√≠bulos son capaces de regular la densidad en 
la vecindad de la puerta de salida. La presi¬¥on disminuye porque los vest√≠bulos obligan a la multitud a 
dispersarse m√°s."


Contexto: {context}

Pregunta: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
setup = RunnableParallel(context=vectorstore.as_retriever(), question=RunnablePassthrough())
chain = (
    setup   
    | prompt
    | model
    | parser
)


#### Correr el programa en el front ####

if pregunta:
    response = chain.invoke(pregunta)
    st.write(response)

st.markdown("""---""")
st.write("#### Preguntas de ejemplo:")
st.write("- ¬øDe qu√© trata la tesis?")
st.write("- ¬øQu√© es la din√°mica peatonal?")
st.write("- ¬øQu√© par√°metros se ajustaron y cu√°les son los valores √≥ptimos?")
st.write("- ¬øCu√°l es la f√≥rmula matem√°tica de los blocking clusters?")
st.write("- ¬øC√≥mo se pueden mejorar las evacuaciones de emergencia ?")

st.markdown("""---""")
st.write("Enlace a la tesis: https://bibliotecadigital.exactas.uba.ar/download/tesis/tesis_n7313_Sticco.pdf")

